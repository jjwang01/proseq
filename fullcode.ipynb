{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyBigWig\n",
    "import math\n",
    "k562_positive=pyBigWig.open('/oak/stanford/groups/akundaje/laks/proseq_chromputer/K562_unt.sort.bed.gz_plus.bw')\n",
    "k562_negative=pyBigWig.open('/oak/stanford/groups/akundaje/laks/proseq_chromputer/K562_unt.sort.bed.gz_minus.bw')\n",
    "h3k27ac_bigwig=pyBigWig.open('/oak/stanford/groups/akundaje/laks/proseq_chromputer/wgEncodeBroadHistoneK562H3k27acStdSig.bigWig')\n",
    "h3k27me3_bigwig=pyBigWig.open('/oak/stanford/groups/akundaje/laks/proseq_chromputer/wgEncodeBroadHistoneK562H3k27me3StdSig.bigWig')\n",
    "h3k4me3_bigwig=pyBigWig.open('/oak/stanford/groups/akundaje/laks/proseq_chromputer/wgEncodeBroadHistoneK562H3k4me3StdSig.bigWig')\n",
    "h3k4me1_bigwig=pyBigWig.open('/oak/stanford/groups/akundaje/laks/proseq_chromputer/wgEncodeBroadHistoneK562H3k4me1StdSig.bigWig')\n",
    "h3k27ac_peak_replicated_bed=pd.read_csv('/oak/stanford/groups/akundaje/laks/proseq_chromputer/wgEncodeBroadHistoneK562H3k27acStdPk.broadPeak',sep='\\t',header=None)\n",
    "h3k27ac_peak_replicated_bed['diff']=h3k27ac_peak_replicated_bed[2]-h3k27ac_peak_replicated_bed[1]\n",
    "h3k27ac_peak_replicated_bed['absolute_diff']=3000-h3k27ac_peak_replicated_bed['diff']\n",
    "h3k27ac_peak_replicated_bed['left']=h3k27ac_peak_replicated_bed['absolute_diff'].apply(lambda x : math.floor(float(x)/2))\n",
    "h3k27ac_peak_replicated_bed['right']=h3k27ac_peak_replicated_bed['absolute_diff'].apply(lambda x : math.ceil(float(x)/2))\n",
    "h3k27ac_peak_replicated_bed['left_new']=h3k27ac_peak_replicated_bed[1]-h3k27ac_peak_replicated_bed['left']\n",
    "h3k27ac_peak_replicated_bed['right_new']=h3k27ac_peak_replicated_bed[2]+h3k27ac_peak_replicated_bed['right']\n",
    "h3k27ac_peak_replicated_bed['new_diff']=h3k27ac_peak_replicated_bed['right_new']-h3k27ac_peak_replicated_bed['left_new']\n",
    "h3k27ac_peak_replicated_bed['left_new']=h3k27ac_peak_replicated_bed['left_new'].astype(int)\n",
    "h3k27ac_peak_replicated_bed['right_new']=h3k27ac_peak_replicated_bed['right_new'].astype(int)\n",
    "#h3k27ac_peak_replicated_bed[[0,'left_new','right_new']].to_csv('/oak/stanford/groups/akundaje/laks/proseq_chromputer/h3k27ac_peak_list.bed',sep='\\t',header=None,index=False)\n",
    "#os.system('bedtools makewindows -g /oak/stanford/groups/akundaje/laks/proseq_chromputer/hg19.chrom.sizes -w 2000 -s 200 > /oak/stanford/groups/akundaje/laks/proseq_chromputer/genome.2kb.bed')\n",
    "#os.system('bedtools subtract -a /oak/stanford/groups/akundaje/laks/proseq_chromputer/genome.2kb.bed -b /oak/stanford/groups/akundaje/laks/proseq_chromputer/h3k27ac_peak_list.bed > /oak/stanford/groups/akundaje/laks/proseq_chromputer/genome.2kb_h3k27ac_k562_hg19_negatives.bed')\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "total_sum=0\n",
    "for i in range(0,22):\n",
    "\tif i==0:\n",
    "\t\tchrm='chrX'\n",
    "\telse:\n",
    "\t\tchrm='chr'+str(i)\n",
    "\tlength=k562_positive.chroms(chrm)\n",
    "\tvalues=k562_positive.values(chrm,0,length)\n",
    "\ta=np.array(values)\n",
    "\ta[np.isnan(a)] = 0\n",
    "\ttotal_sum=total_sum+np.sum(a)\n",
    "\n",
    "total_sum_negative=0\n",
    "for i in range(0,22):\n",
    "\tif i==0:\n",
    "\t\tchrm='chrX'\n",
    "\telse:\n",
    "\t\tchrm='chr'+str(i)\n",
    "\tlength=k562_negative.chroms(chrm)\n",
    "\tvalues=k562_negative.values(chrm,0,length)\n",
    "\ta=np.array(values)\n",
    "\ta[np.isnan(a)] = 0\n",
    "\ttotal_sum_negative=total_sum_negative+np.sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the positive peak file\n",
    "h3k27ac_positives=h3k27ac_peak_replicated_bed[[0,'left_new','right_new']]\n",
    "h3k27ac_positives.columns=['chr','start','end']\n",
    "#\n",
    "h3k27ac_positives_validation=h3k27ac_positives[h3k27ac_positives['chr']=='chr22']\n",
    "h3k27ac_positives_testing=h3k27ac_positives[h3k27ac_positives['chr']=='chr19']\n",
    "#\n",
    "h3k27ac_positives=h3k27ac_positives[h3k27ac_positives['chr'].isin(['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10','chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr20','chr21','chrX'])]\n",
    "h3k27ac_positives['id']=h3k27ac_positives['chr'].astype(str)+'_'+h3k27ac_positives['start'].astype(str)+'_'+h3k27ac_positives['end'].astype(str)\n",
    "#\n",
    "#h3k27ac_negatives=pd.read_csv('/oak/stanford/groups/akundaje/laks/proseq_chromputer/genome.2kb_h3k27ac_k562_hg19_negatives.bed',sep='\\t',header=None)\n",
    "#h3k27ac_negatives=h3k27ac_negatives[h3k27ac_negatives[1]>20000]\n",
    "#h3k27ac_negatives.columns=['chr','start','end']\n",
    "#h3k27ac_negatives=h3k27ac_negatives[h3k27ac_negatives['end']-h3k27ac_negatives['start']==2000]\n",
    "#\n",
    "#h3k27ac_negatives_validation=h3k27ac_negatives[h3k27ac_negatives['chr']=='chr22']\n",
    "#\n",
    "#h3k27ac_negatives['id']=h3k27ac_negatives['chr'].astype(str)+'_'+h3k27ac_negatives['start'].astype(str)+'_'+h3k27ac_negatives['end'].astype(str)\n",
    "#h3k27ac_negatives=h3k27ac_negatives[h3k27ac_negatives['chr'].isin(['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10','chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19','chr20','chr21','chrX'])]\n",
    "#\n",
    "#validation_data=h3k27ac_positives_validation.append(h3k27ac_negatives_validation)\n",
    "#print validation_data[0:5]\n",
    "validation_data=h3k27ac_positives_validation\n",
    "test_data=h3k27ac_positives_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_width=13000 \n",
    "input_dimension=1\n",
    "number_of_convolutions=16\n",
    "filters=[32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32]\n",
    "filter_dim=[11,11,11,11,11,11,11,11,21,21,21,21,41,41,41,41]\n",
    "dilation=[1,1,1,1,4,4,4,4,10,10,10,10,25,25,25,25]\n",
    "activations='relu'\n",
    "bn_true=True\n",
    "\n",
    "\n",
    "def BatchNormalization_mod(conv, bn_flag=True):\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "    if bn_flag:\n",
    "        return BatchNormalization()(conv)\n",
    "    else:\n",
    "        return conv\n",
    "\n",
    "\n",
    "def res_block(conv,num_filter,f_width,act,d_rate,i,bn_true=True):\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    from keras import backend as K\n",
    "    from keras.layers.pooling import GlobalMaxPooling1D,MaxPooling2D,MaxPooling1D\n",
    "    from keras.models import Sequential,Model\n",
    "    from keras.layers import Dense,Activation,Dropout,Flatten,Reshape,Input, Embedding, LSTM, Dense,Concatenate\n",
    "    from keras.layers.convolutional import Conv1D,Conv2D,Cropping1D\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "    from keras.regularizers import l1,l2\n",
    "    from keras.optimizers import SGD,RMSprop,Adam\n",
    "    from sklearn.metrics import average_precision_score\n",
    "    crop_id=Cropping1D(d_rate*(f_width-1))(conv)\n",
    "    conv1 = BatchNormalization_mod(conv,bn_true)\n",
    "    conv1 = Activation(\"relu\")(conv1)\n",
    "    conv1 = Conv1D(num_filter,f_width,dilation_rate=d_rate,padding=\"valid\",name='conv_'+str(i)+'_a')(conv1)\n",
    "    conv1 = BatchNormalization_mod(conv1,bn_true)\n",
    "    conv1 = Activation(\"relu\")(conv1)\n",
    "    conv1 = Conv1D(num_filter,f_width,dilation_rate=d_rate,padding=\"valid\",name='conv_'+str(i)+'_b')(conv1)\n",
    "    return keras.layers.Add()([conv1, crop_id])\n",
    "\n",
    "def build1d_model_residual(input_width,input_dimension,number_of_convolutions,filters,filter_dim,dilation,activations,bn_true=True,max_flag=True):\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    from keras import backend as K\n",
    "    from keras.layers.pooling import GlobalMaxPooling1D,MaxPooling2D,MaxPooling1D\n",
    "    from keras.models import Sequential,Model\n",
    "    from keras.layers import Dense,Activation,Dropout,Flatten,Reshape,Input, Embedding, LSTM, Dense,Concatenate\n",
    "    from keras.layers.convolutional import Conv1D,Conv2D\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "    from keras.regularizers import l1,l2\n",
    "    from keras.optimizers import SGD,RMSprop,Adam\n",
    "    from sklearn.metrics import average_precision_score\n",
    "    input1=Input(shape=(13000,1), name='proseq_positive')\n",
    "    input2=Input(shape=(13000,1), name='proseq_negative')\n",
    "    inputlayer=keras.layers.add()([input1, input2])\n",
    "    conv=Conv1D(32,1, padding='same',activation='relu',name = 'upsampling')(inputlayer)\n",
    "    for i in range(0,number_of_convolutions):\n",
    "            conv = res_block(conv,filters[i],filter_dim[i],activations,dilation[i],i,bn_true)\n",
    "    conv= Conv1D(32, 1,padding='valid', activation='relu',name='down_sampling')(conv)\n",
    "    output=Conv1D(1,1,activation='relu',name='output')(conv)\n",
    "    model = Model(input=[input1,input2],output=[output])\n",
    "    model.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def validation_loss(model,validation_data):\n",
    "    validation_mse=[]\n",
    "    for i in validation_data.values:\n",
    "            positive_values=np.nan_to_num(k562_positive.values(i[0],i[1]-5000,i[2]+5000))/(total_sum+np.abs(total_sum_negative))\n",
    "            negative_values=np.nan_to_num(k562_negative.values(i[0],i[1]-5000,i[2]+5000))/(total_sum+np.abs(total_sum_negative))\n",
    "            h3k27ac_value=np.arcsinh(np.nan_to_num(h3k27ac_bigwig.values(i[0],i[1],i[2])))\n",
    "            loss_validation=model.evaluate({'proseq_positive':np.expand_dims(np.expand_dims(np.asarray(positive_values),1),0),\\\n",
    "                'proseq_negative':np.expand_dims(np.expand_dims(np.asarray(negative_values),1),0) },\\\n",
    "                {'output':np.expand_dims(np.expand_dims(np.nan_to_num(np.asarray(h3k27ac_value)),1),0)},\\\n",
    "                batch_size=1,verbose=0)\n",
    "            validation_mse.append(loss_validation[0])\n",
    "    return float(sum(validation_mse))/len(validation_mse)\n",
    "\n",
    "\n",
    "def calculate_input(X_train):\n",
    "    xtrain_positive=[]\n",
    "    xtrain_negative=[]\n",
    "    ytrain=[]\n",
    "    for i in X_train.values:\n",
    "        try:\n",
    "        \txtrain_positive.append(np.nan_to_num(k562_positive.values(i[0],int(i[1])-5000,int(i[2])+5000))/(total_sum+np.abs(total_sum_negative) ))\n",
    "        \txtrain_negative.append(np.nan_to_num(k562_negative.values(i[0],int(i[1])-5000,int(i[2])+5000))/(total_sum+np.abs(total_sum_negative)))\n",
    "        \tytrain.append(np.arcsinh(np.nan_to_num(h3k27ac_bigwig.values(i[0],int(i[1]),int(i[2])))))\n",
    "    \texcept Exception as e:\n",
    "    \t\tprint str(e)\n",
    "    \t\tprint i\n",
    "    xtrain_positive=np.expand_dims(np.asarray(xtrain_positive),2)\n",
    "    xtrain_negative=np.expand_dims(np.asarray(xtrain_negative),2)\n",
    "    ytrain=np.expand_dims(np.asarray(ytrain),2)\n",
    "    return (xtrain_positive,xtrain_negative,ytrain)\n",
    "\n",
    "\n",
    "model=build1d_model_residual(input_width,input_dimension,number_of_convolutions,filters,filter_dim,dilation,activations,bn_true)\n",
    "validation_loss_val=10000000\n",
    "patience=0\n",
    "while True:\n",
    "    used_positives=pd.Series()\n",
    "    #used_negatives=pd.Series()\n",
    "    flag=0\n",
    "    while True:\n",
    "        try:\n",
    "            peaks_regions=h3k27ac_positives[~(h3k27ac_positives['id'].isin(used_positives))].sample(n=150,random_state=95918)\n",
    "        except:\n",
    "            used_positives=pd.Series()\n",
    "            print used_positives.shape\n",
    "            #print used_negatives.shape\n",
    "            flag=flag+1\n",
    "            if flag ==1:\n",
    "            \tbreak\n",
    "            #peaks_regions=h3k27ac_positives[~(h3k27ac_positives['id'].isin(used_positives))].sample(n=150,random_state=95918)\n",
    "            print '#########################breaking#################'\n",
    "            break\n",
    "        #try:\n",
    "        #    nonpeaks_regions=h3k27ac_negatives[~(h3k27ac_negatives['id'].isin(used_negatives))].sample(n=150,random_state=95918)\n",
    "        #except:\n",
    "        #    used_negatives=pd.Series()\n",
    "        used_positives=used_positives.append(peaks_regions['id'])\n",
    "        #used_negatives=used_negatives.append(nonpeaks_regions['id'])\n",
    "        #print used_positives.shape\n",
    "        #print used_negatives.shape\n",
    "        #X_train=peaks_regions.append(nonpeaks_regions)\n",
    "        X_train=peaks_regions\n",
    "        X_train=X_train.sample(frac=1)\n",
    "        X_train_positive,X_train_negative,Y_train_histone=calculate_input(X_train)\n",
    "        a=model.fit({'proseq_positive': X_train_positive , 'proseq_negative': X_train_negative},{'output':Y_train_histone},epochs=1, batch_size=50,verbose=0)    \n",
    "    new_validation_loss_val=validation_loss(model,validation_data)\n",
    "    if new_validation_loss_val < validation_loss_val:\n",
    "        print '#############change in value ##############'\n",
    "        print new_validation_loss_val\n",
    "        print validation_loss_val\n",
    "        print '#############change in value ##############'\n",
    "        validation_loss_val=new_validation_loss_val\n",
    "        model.save_weights('/oak/stanford/groups/akundaje/laks/proseq_chromputer/13kb_3kb_h3k27ac_ADD.hdf5')\n",
    "    else:\n",
    "    \tprint '############# no change in value ##############'\n",
    "        print new_validation_loss_val\n",
    "        print validation_loss_val\n",
    "        print '############# no change in value ##############'\n",
    "    \tmodel.load_weights('/oak/stanford/groups/akundaje/laks/proseq_chromputer/13kb_3kb_h3k27ac_ADD.hdf5')\n",
    "        patience=patience+1\n",
    "    if patience==5:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
